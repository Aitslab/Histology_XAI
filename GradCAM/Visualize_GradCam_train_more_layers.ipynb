{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grad-CAM\n",
    "In this notebook we calculate Gradcam and activation map of our two best CNN models (effnetB4 and Vgg16) where we trained more layers from top (go deeper).\n",
    "For Vgg16 we train once one more convolutional layer and once two more convolutional layers on top of top layers.\n",
    "For effnet we trained once batchnormalization layer, once convolutional layer and once two layers (BN and Conv) together on top \n",
    "of top layers.\n",
    "\n",
    "\n",
    "Written by\n",
    "### Salma Kazemi Rashed\n",
    "The code is partly taken from https://arxiv.org/abs/1610.02391 paper and revised for our model and script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##libraries \n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, BatchNormalization, Dropout,LeakyReLU\n",
    "from tensorflow.keras.models import Model, model_from_json\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D,MaxPool2D\n",
    "\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.applications import EfficientNetB0,EfficientNetB4,EfficientNetB7\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from tensorflow.keras.models import load_model\n",
    "import seaborn as sns\n",
    "#############\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This class was defined in our model for 3 class classifier\n",
    "### Define again here \n",
    "class MulticlassTruePositives(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='multiclass_true_positives', **kwargs):\n",
    "        super(MulticlassTruePositives, self).__init__(name=name, **kwargs)\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.reshape(tf.argmax(y_pred, axis=1), shape=(-1, 1))\n",
    "        values = tf.cast(y_true, 'int32') == tf.cast(y_pred, 'int32')\n",
    "        values = tf.cast(values, 'float32')\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, 'float32')\n",
    "            values = tf.multiply(values, sample_weight)\n",
    "        self.true_positives.assign_add(tf.reduce_sum(values))\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives\n",
    "\n",
    "    def reset_state(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.true_positives.assign(0.)\n",
    "###########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is for train more layers from VGG model with LeakyRelU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_DIR = ['2nd_part']#['1st_part','2nd_part','3rd_part']\n",
    "MODELS         = '../../models/train_more_layers/vgg_aug_3fold_blurred_2_layer_LeakyReLU/'\n",
    "SAMPLES        = '../../results/Grad_cam/Train_more_layers/samples_V_r_a_blurred_2layers_LeakyReLU/'\n",
    "LAYER_TYPE     =  LeakyReLU(0.2)\n",
    "LAYER_TYPE_2   =  LeakyReLU(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "###########################################################################################\n",
    "### We had 3 folds here each as validation set\n",
    "val_dirs = VALIDATION_DIR\n",
    "###############################################\n",
    "### We went through all folds and check 3 models \n",
    "\n",
    "\n",
    "\n",
    "for val_dir in val_dirs:\n",
    "    ############################# Functional model ##################################################\n",
    "    inputs = tf.keras.Input(shape=(224,224,3))  ## This is very important to define functional API \n",
    "    ## Base model (VGG16)\n",
    "    x = VGG16(                                \n",
    "      input_tensor=inputs, \n",
    "      include_top=False,\n",
    "      weights='imagenet')  ## Pretrained weights\n",
    "\n",
    "    x.trainable = False  ## Layers are not trainable\n",
    "\n",
    "    ## Change top layers\n",
    "    layer_type = LAYER_TYPE\n",
    "    layer_type_fc2  = LAYER_TYPE_2\n",
    "    x = tf.keras.layers.Flatten(name='flatten')(x.output)\n",
    "    x = tf.keras.layers.Dense(128, activation=layer_type, name='fc1')(x)\n",
    "    x = tf.keras.layers.Dense(128, activation=layer_type_fc2, name='fc2')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5, name='dropout')(x)\n",
    "    x = tf.keras.layers.Dense(3, activation='softmax', name='predictions')(x)\n",
    "    \n",
    "    '''\n",
    "    block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
    "                                                                 \n",
    "    block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
    "                                                                 \n",
    "    flatten (Flatten)           (None, 25088)             0         \n",
    "                                                                 \n",
    "    fc1 (Dense)                 (None, 128)               3211392   \n",
    "                                                                 \n",
    "    fc2 (Dense)                 (None, 128)               16512     \n",
    "                                                                 \n",
    "    dropout (Dropout)           (None, 128)               0         \n",
    "                                                                 \n",
    "    predictions (Dense)         (None, 3)                 387       \n",
    "                                                                 \n",
    "    =================================================================\n",
    "    Total params: 17,942,979\n",
    "    Trainable params: 3,228,291\n",
    "    Non-trainable params: 14,714,688\n",
    "    _________________________________________________________________\n",
    "    '''\n",
    "    ## Here we define metrics for multi-class classifier\n",
    "    METRICS = [\n",
    "      keras.metrics.SparseCategoricalAccuracy(),\n",
    "      MulticlassTruePositives()]\n",
    " \n",
    "    pretrained_model = tf.keras.Model(inputs, x)\n",
    "    \n",
    "    \n",
    "    ## Compile model \n",
    "    pretrained_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=METRICS)\n",
    "    ## After defining the functional model we will have a model loaded from weights\n",
    "    \n",
    "    \n",
    "    \n",
    "    ############################ Loaded model from Weights #############################################\n",
    "    ##weights saved before \n",
    "    weights = glob.glob(MODELS+'*_best_weights.*')[0]\n",
    "    \n",
    "    ### This is the model created from weights\n",
    "    loaded_1 = keras.models.load_model(\n",
    "        weights, custom_objects={\"MulticlassTruePositives\": MulticlassTruePositives,'Functional':VGG16()})\n",
    "    \n",
    "    #loaded_1.summary()\n",
    "    weights_list = loaded_1.get_weights()\n",
    "   \n",
    "    \n",
    "    ## 2. load matched weights\n",
    "    #### The layer index differ in weighted and functional models ########################################\n",
    "    pretrained_model.layers[-1].set_weights([weights_list[-2],weights_list[-1]])\n",
    "    pretrained_model.layers[-3].set_weights([weights_list[-4],weights_list[-3]])\n",
    "    pretrained_model.layers[-4].set_weights([weights_list[-6],weights_list[-5]])\n",
    "    pretrained_model.layers[-7].set_weights([weights_list[-8],weights_list[-7]])\n",
    "    pretrained_model.layers[-8].set_weights([weights_list[-10],weights_list[-9]])\n",
    "    #pretrained_model.layers[-9].set_weights([weights_list[-12],weights_list[-11]])\n",
    "    \n",
    "    \n",
    "    \n",
    "    model=pretrained_model\n",
    "    ##  3. define the network with new probes \n",
    "    ################## Define a partial model with new probes from fully connected layers as output###############\n",
    "    gradModel = Model(\n",
    "          inputs=[pretrained_model.inputs],\n",
    "          outputs=[pretrained_model.get_layer('block5_conv3').output,pretrained_model.get_layer('fc1').output,\n",
    "                   pretrained_model.get_layer('fc2').output,\n",
    "                   pretrained_model.get_layer('dropout').output,\n",
    "                   pretrained_model.output])\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### This is for calculating the gradients or sensitivity manually\n",
    "    #TestGradientModel  = Model(\n",
    "    #                     inputs  =[pretrained_model.get_layer('block5_conv3').output],\n",
    "    #                     outputs =[pretrained_model.output])\n",
    "    \n",
    "    ####load sample images for calcuating gradcam matrix\n",
    "    ######### Sample image directory is defined by model name#############\n",
    "    ### We wanted to stitch small tiles to get the bigger image\n",
    "    \n",
    "    \n",
    "    \n",
    "    imgs = glob.glob(SAMPLES+'main_img_'+val_dir+'/*.tif')\n",
    "    \n",
    "    ## repeat for all images\n",
    "    \n",
    "    for im in imgs:\n",
    "        img_path = im\n",
    "        \n",
    "        if cv2.imread(img_path) is not None: ## Check if the image exists\n",
    "            input_img = cv2.imread(img_path)\n",
    "            \n",
    "            max_tile_i = int(np.floor(input_img.shape[0]/224))  ##maxiumum number of rows\n",
    "            max_tile_j = int(np.floor(input_img.shape[1]/224))  ##maxiumum number of columns\n",
    "            \n",
    "            ##initialization\n",
    "            Heatmap_2D   = np.zeros([max_tile_i*224,max_tile_j*224])\n",
    "            Heatmap_only = np.zeros([max_tile_i*224,max_tile_j*224,1])\n",
    "            Gradcam_img  = np.zeros([max_tile_i*224,max_tile_j*224,3])\n",
    "            \n",
    "            ### We need to feed the tiles into model to get the activation map\n",
    "            for i in range(max_tile_i):\n",
    "                for j in range(max_tile_j):\n",
    "                    tmp_img = input_img[224*i:224*(i+1),224*j:224*(j+1),:]\n",
    "                    tmp_img = cv2.resize(tmp_img,(224,224))\n",
    "                    y = np.expand_dims(tmp_img, axis=0)  \n",
    "                    img_array = y\n",
    "\n",
    "                    ## 4. calculate gradcam\n",
    "                    \n",
    "                    ## We need to calculate the gradient of output loss w.r.t the prob layer\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        # cast the image tensor to a float-32 data type, pass the\n",
    "                        # forward propagate the image through the gradient model, and grab the loss\n",
    "                        # associated with the specific class index\n",
    "                        \n",
    "                        ## The input image\n",
    "                        inputs = tf.cast(img_array, tf.float32)\n",
    "                        ## The probes as last conv layers, fully connected layers and predictions \n",
    "                        convOutputs, fc1,fc2,dropout,predictions = gradModel(inputs)  ## We just took the prob we defined before\n",
    "\n",
    "                        \n",
    "                        \n",
    "                        ### Loss is the final output probablity before softmax layer\n",
    "                        loss = predictions[:, np.argmax(predictions)] ## the predicted class always\n",
    "                        img_title = 'predicted score: '+ str( np.argmax(predictions))\n",
    "                        \n",
    "                        \n",
    "                        # use automatic differentiation to compute the gradients\n",
    "                        grads = tape.gradient(loss, convOutputs) ## we can calculate this manually\n",
    "                        ### by changing each element in the (14, 14, 512 ) map calculate the diff in the loss (dy/dx)\n",
    "                        #print(grads.shape) = (14,14,512) in VGG16\n",
    "                        ##################################################################################\n",
    "                        ## manual calculation \n",
    "                         \n",
    "                        #d_loss    = TestGradientModel(convOutputs)\n",
    "                        #ind    = np.argmax(predictions)\n",
    "                        #loss = predictions[:, 0]\n",
    "                        \n",
    "                        #new_grads = tf.reshape(grads,(1,14*14*512)).numpy()\n",
    "                        #new_conv  = tf.reshape(convOutputs,(1,14*14*512))\n",
    "                        #new_shape_grads = tf.reshape(new_grads,(1,14*14*512))\n",
    "                        \n",
    "                        #for n_c in range(len(new_conv[0,:])):\n",
    "                        #    temp_conv  = new_conv.numpy()\n",
    "                        #    temp_conv[0,n_c] = new_conv[0][n_c]*1.5+1e-5\n",
    "                        #    temp_conv   = tf.reshape(temp_conv,(1,14,14,512))\n",
    "                        #    d_loss_d_conv = (TestGradientModel(temp_conv)[:,0]-loss)/(new_conv[0,n_c]*0.5+1e-5)\n",
    "                        #    new_grads[0,n_c] = d_loss_d_conv   \n",
    "                        #new_grads = tf.reshape(new_grads,(1,14,14,512))\n",
    "                        ##################################################################################\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        ##################################################################################\n",
    "                    \n",
    "\n",
    "                        #compute the guided gradients (This is for taking only positive part)\n",
    "                        castConvOutputs = tf.cast(convOutputs > 0, \"float32\")##\n",
    "                        castGrads = tf.cast(grads > 0, \"float32\")\n",
    "                        #print(castConvOutputs.shape) =  ( 1,14,14,512 )\n",
    "                        #print(castGrads.shape)     =   (1,14,14,512)\n",
    "                        guidedGrads = castConvOutputs * castGrads * grads \n",
    "                        #print(guidedGrads.shape)   = (1,14,14,512)\n",
    "\n",
    "                      # the convolution and guided gradients have a batch dimension\n",
    "                      # (which we don't need) so let's grab the volume itself and\n",
    "                      # discard the batch\n",
    "                        convOutputs = convOutputs[0]  \n",
    "                        #print(convOutputs.shape)  =  (14,14,512)\n",
    "                        guidedGrads = guidedGrads[0]\n",
    "                        #print(guidedGrads.shape)  =  (14,14,512)\n",
    "\n",
    "                      # compute the average of the gradient values, and using them\n",
    "                      # as weights, compute the ponderation of the filters with\n",
    "                      # respect to the weights\n",
    "                        weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n",
    "                        #print(weights.shape)  = 512  (took mean over axis 0  and 1 (14,14))\n",
    "\n",
    "                        \n",
    "                        \n",
    "                        cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n",
    "                        #print(cam.shape) = 512 * (14, 14, 512)  = (14,14)\n",
    "                      # grab the spatial dimensions of the input image and resize\n",
    "                      # the output class activation map to match the input image dimensions\n",
    "                        \n",
    "                       \n",
    "                        (w, h) = (img_array.shape[2], img_array.shape[1])  ## (224,224) here\n",
    "                        heatmap = cv2.resize(cam.numpy(), (w, h))    ## resize (14,14) to (224,224) (upsample)\n",
    "                        \n",
    "                         ### It is good to save the tensors for further analysis\n",
    "                        with open(SAMPLES+'0_'+'loss_check_linear.txt','a') as file:\n",
    "                            print(convOutputs, fc1,fc2,dropout,predictions,loss, grads,castConvOutputs,convOutputs,guidedGrads,\\\n",
    "                                  weights, cam, w,h, sep='\\n' ,file = file)\n",
    "                        \n",
    "                        # normalization\n",
    "                        numer = heatmap - np.min(heatmap)            \n",
    "                        denom = (heatmap.max() - heatmap.min())\n",
    "                        if denom == 0:\n",
    "                            denom = denom + 0.0000000000000001\n",
    "                        heatmap = 255*numer / denom\n",
    "\n",
    "                        Heatmap_2D[224*i:224*(i+1),224*j:224*(j+1)] = heatmap[:,:]\n",
    "                        ## big heatmap made of smaller tiles\n",
    "                        \n",
    "                        \n",
    "                        stacked_img = np.stack((heatmap,), axis=-1) ## \n",
    "                        im_color = cv2.applyColorMap(stacked_img.astype(np.uint8), cv2.COLORMAP_JET) ## add a color map\n",
    "                        im_color = cv2.cvtColor(im_color, cv2.COLOR_BGR2RGB)\n",
    "                        Heatmap_only[224*i:224*(i+1),224*j:224*(j+1)] = stacked_img[:,:,:] \n",
    "                        \n",
    "                        \n",
    "                        ## add heatmap to image (0.7 image and 0.3 activation heatmap)\n",
    "                        super_imposed_img = cv2.addWeighted((im_color).astype(np.uint8), 0.3, tmp_img, 0.7,0) \n",
    "                        Gradcam_img[224*i:224*(i+1),224*j:224*(j+1),:] =  super_imposed_img \n",
    "\n",
    "            ### This part is for plot and saving##########################################\n",
    "            \n",
    "            ### Plotting only heatmap\n",
    "            plt.figure(figsize=(15,15))\n",
    "            plt.imshow(Heatmap_2D)\n",
    "            plt.savefig(img_path+'_heatmap.png')\n",
    "           \n",
    "            ### Plotting 3D plot of Heatmap as Z axis\n",
    "            fig   = plt.figure(figsize=(10,10))\n",
    "            ax    = plt.axes(projection='3d')\n",
    "            x     = np.arange(0,max_tile_j*224,1)\n",
    "            y     = np.arange(0,max_tile_i*224,1)\n",
    "            X, Y  = np.meshgrid(x, y)\n",
    "            ax.contour3D(X,Y, Heatmap_only[:,:,0], 100,alpha = 0.5, cmap='jet')\n",
    "            ax.set_xlabel('x')\n",
    "            ax.set_ylabel('y')\n",
    "            ax.set_zlabel('z')\n",
    "            ax.view_init(azim=270, elev=-89)\n",
    "            plt.savefig(img_path+'_3D.png')\n",
    "\n",
    "\n",
    "            \n",
    "            ### Plotting image with GRAD-CAM filter added \n",
    "            plt.figure(figsize=(10,10))\n",
    "            plt.imshow(Gradcam_img.astype(np.uint8))\n",
    "            norm = mpl.colors.Normalize(vmin=Gradcam_img.min(), vmax=Gradcam_img.max())\n",
    "            plt.savefig(img_path+'_img_filter.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_DIR = ['2nd_part']#['1st_part','2nd_part','3rd_part']\n",
    "MODELS         = '../../models/train_more_layers/vgg_aug_3fold_blurred_3_layer_relu/'\n",
    "SAMPLES        = '../../results/Grad_cam/Train_more_layers/samples_V_r_a_blurred_3layers_relu/'\n",
    "LAYER_TYPE     =  'relu'\n",
    "LAYER_TYPE_2   =  'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "###########################################################################################\n",
    "### We had 3 folds here each as validation set\n",
    "val_dirs = VALIDATION_DIR\n",
    "###############################################\n",
    "### We went through all folds and check 3 models \n",
    "\n",
    "\n",
    "\n",
    "for val_dir in val_dirs:\n",
    "    ############################# Functional model ##################################################\n",
    "    inputs = tf.keras.Input(shape=(224,224,3))  ## This is very important to define functional API \n",
    "    ## Base model (VGG16)\n",
    "    x = VGG16(                                \n",
    "      input_tensor=inputs, \n",
    "      include_top=False,\n",
    "      weights='imagenet')  ## Pretrained weights\n",
    "\n",
    "    x.trainable = False  ## Layers are not trainable\n",
    "\n",
    "    ## Change top layers\n",
    "    layer_type = LAYER_TYPE\n",
    "    layer_type_fc2  = LAYER_TYPE_2\n",
    "    x = tf.keras.layers.Flatten(name='flatten')(x.output)\n",
    "    x = tf.keras.layers.Dense(128, activation=layer_type, name='fc1')(x)\n",
    "    x = tf.keras.layers.Dense(128, activation=layer_type_fc2, name='fc2')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5, name='dropout')(x)\n",
    "    x = tf.keras.layers.Dense(3, activation='softmax', name='predictions')(x)\n",
    "    \n",
    "    '''\n",
    "    block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
    "                                                                 \n",
    "    block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
    "                                                                 \n",
    "    flatten (Flatten)           (None, 25088)             0         \n",
    "                                                                 \n",
    "    fc1 (Dense)                 (None, 128)               3211392   \n",
    "                                                                 \n",
    "    fc2 (Dense)                 (None, 128)               16512     \n",
    "                                                                 \n",
    "    dropout (Dropout)           (None, 128)               0         \n",
    "                                                                 \n",
    "    predictions (Dense)         (None, 3)                 387       \n",
    "                                                                 \n",
    "    =================================================================\n",
    "    Total params: 17,942,979\n",
    "    Trainable params: 3,228,291\n",
    "    Non-trainable params: 14,714,688\n",
    "    _________________________________________________________________\n",
    "    '''\n",
    "    ## Here we define metrics for multi-class classifier\n",
    "    METRICS = [\n",
    "      keras.metrics.SparseCategoricalAccuracy(),\n",
    "      MulticlassTruePositives()]\n",
    " \n",
    "    pretrained_model = tf.keras.Model(inputs, x)\n",
    "    \n",
    "    \n",
    "    ## Compile model \n",
    "    pretrained_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=METRICS)\n",
    "    ## After defining the functional model we will have a model loaded from weights\n",
    "    \n",
    "    \n",
    "    \n",
    "    ############################ Loaded model from Weights #############################################\n",
    "    ##weights saved before \n",
    "    weights = glob.glob(MODELS+'*_best_weights.*')[0]\n",
    "    \n",
    "    ### This is the model created from weights\n",
    "    loaded_1 = keras.models.load_model(\n",
    "        weights, custom_objects={\"MulticlassTruePositives\": MulticlassTruePositives,'Functional':VGG16()})\n",
    "    \n",
    "    #loaded_1.summary()\n",
    "    weights_list = loaded_1.get_weights()\n",
    "   \n",
    "    \n",
    "    ## 2. load matched weights\n",
    "    #### The layer index differ in weighted and functional models ########################################\n",
    "    pretrained_model.layers[-1].set_weights([weights_list[-2],weights_list[-1]])\n",
    "    pretrained_model.layers[-3].set_weights([weights_list[-4],weights_list[-3]])\n",
    "    pretrained_model.layers[-4].set_weights([weights_list[-6],weights_list[-5]])\n",
    "    pretrained_model.layers[-7].set_weights([weights_list[-8],weights_list[-7]])\n",
    "    pretrained_model.layers[-8].set_weights([weights_list[-10],weights_list[-9]])\n",
    "    pretrained_model.layers[-9].set_weights([weights_list[-12],weights_list[-11]])\n",
    "    \n",
    "    \n",
    "    \n",
    "    model=pretrained_model\n",
    "    ##  3. define the network with new probes \n",
    "    ################## Define a partial model with new probes from fully connected layers as output###############\n",
    "    gradModel = Model(\n",
    "          inputs=[pretrained_model.inputs],\n",
    "          outputs=[pretrained_model.get_layer('block5_conv3').output,pretrained_model.get_layer('fc1').output,\n",
    "                   pretrained_model.get_layer('fc2').output,\n",
    "                   pretrained_model.get_layer('dropout').output,\n",
    "                   pretrained_model.output])\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### This is for calculating the gradients or sensitivity manually\n",
    "    #TestGradientModel  = Model(\n",
    "    #                     inputs  =[pretrained_model.get_layer('block5_conv3').output],\n",
    "    #                     outputs =[pretrained_model.output])\n",
    "    \n",
    "    ####load sample images for calcuating gradcam matrix\n",
    "    ######### Sample image directory is defined by model name#############\n",
    "    ### We wanted to stitch small tiles to get the bigger image\n",
    "    \n",
    "    \n",
    "    \n",
    "    imgs = glob.glob(SAMPLES+'main_img_'+val_dir+'/*.tif')\n",
    "    \n",
    "    ## repeat for all images\n",
    "    \n",
    "    for im in imgs:\n",
    "        img_path = im\n",
    "        \n",
    "        if cv2.imread(img_path) is not None: ## Check if the image exists\n",
    "            input_img = cv2.imread(img_path)\n",
    "            \n",
    "            max_tile_i = int(np.floor(input_img.shape[0]/224))  ##maxiumum number of rows\n",
    "            max_tile_j = int(np.floor(input_img.shape[1]/224))  ##maxiumum number of columns\n",
    "            \n",
    "            ##initialization\n",
    "            Heatmap_2D   = np.zeros([max_tile_i*224,max_tile_j*224])\n",
    "            Heatmap_only = np.zeros([max_tile_i*224,max_tile_j*224,1])\n",
    "            Gradcam_img  = np.zeros([max_tile_i*224,max_tile_j*224,3])\n",
    "            \n",
    "            ### We need to feed the tiles into model to get the activation map\n",
    "            for i in range(max_tile_i):\n",
    "                for j in range(max_tile_j):\n",
    "                    tmp_img = input_img[224*i:224*(i+1),224*j:224*(j+1),:]\n",
    "                    tmp_img = cv2.resize(tmp_img,(224,224))\n",
    "                    y = np.expand_dims(tmp_img, axis=0)  \n",
    "                    img_array = y\n",
    "\n",
    "                    ## 4. calculate gradcam\n",
    "                    \n",
    "                    ## We need to calculate the gradient of output loss w.r.t the prob layer\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        # cast the image tensor to a float-32 data type, pass the\n",
    "                        # forward propagate the image through the gradient model, and grab the loss\n",
    "                        # associated with the specific class index\n",
    "                        \n",
    "                        ## The input image\n",
    "                        inputs = tf.cast(img_array, tf.float32)\n",
    "                        ## The probes as last conv layers, fully connected layers and predictions \n",
    "                        convOutputs, fc1,fc2,dropout,predictions = gradModel(inputs)  ## We just took the prob we defined before\n",
    "\n",
    "                        \n",
    "                        \n",
    "                        ### Loss is the final output probablity before softmax layer\n",
    "                        loss = predictions[:, np.argmax(predictions)] ## the predicted class always\n",
    "                        img_title = 'predicted score: '+ str( np.argmax(predictions))\n",
    "                        \n",
    "                        \n",
    "                        # use automatic differentiation to compute the gradients\n",
    "                        grads = tape.gradient(loss, convOutputs) ## we can calculate this manually\n",
    "                        ### by changing each element in the (14, 14, 512 ) map calculate the diff in the loss (dy/dx)\n",
    "                        #print(grads.shape) = (14,14,512) in VGG16\n",
    "                        ##################################################################################\n",
    "                        ## manual calculation \n",
    "                         \n",
    "                        #d_loss    = TestGradientModel(convOutputs)\n",
    "                        #ind    = np.argmax(predictions)\n",
    "                        #loss = predictions[:, 0]\n",
    "                        \n",
    "                        #new_grads = tf.reshape(grads,(1,14*14*512)).numpy()\n",
    "                        #new_conv  = tf.reshape(convOutputs,(1,14*14*512))\n",
    "                        #new_shape_grads = tf.reshape(new_grads,(1,14*14*512))\n",
    "                        \n",
    "                        #for n_c in range(len(new_conv[0,:])):\n",
    "                        #    temp_conv  = new_conv.numpy()\n",
    "                        #    temp_conv[0,n_c] = new_conv[0][n_c]*1.5+1e-5\n",
    "                        #    temp_conv   = tf.reshape(temp_conv,(1,14,14,512))\n",
    "                        #    d_loss_d_conv = (TestGradientModel(temp_conv)[:,0]-loss)/(new_conv[0,n_c]*0.5+1e-5)\n",
    "                        #    new_grads[0,n_c] = d_loss_d_conv   \n",
    "                        #new_grads = tf.reshape(new_grads,(1,14,14,512))\n",
    "                        ##################################################################################\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        ##################################################################################\n",
    "                    \n",
    "\n",
    "                        #compute the guided gradients (This is for taking only positive part)\n",
    "                        castConvOutputs = tf.cast(convOutputs > 0, \"float32\")##\n",
    "                        castGrads = tf.cast(grads > 0, \"float32\")\n",
    "                        #print(castConvOutputs.shape) =  ( 1,14,14,512 )\n",
    "                        #print(castGrads.shape)     =   (1,14,14,512)\n",
    "                        guidedGrads = castConvOutputs * castGrads * grads \n",
    "                        #print(guidedGrads.shape)   = (1,14,14,512)\n",
    "\n",
    "                      # the convolution and guided gradients have a batch dimension\n",
    "                      # (which we don't need) so let's grab the volume itself and\n",
    "                      # discard the batch\n",
    "                        convOutputs = convOutputs[0]  \n",
    "                        #print(convOutputs.shape)  =  (14,14,512)\n",
    "                        guidedGrads = guidedGrads[0]\n",
    "                        #print(guidedGrads.shape)  =  (14,14,512)\n",
    "\n",
    "                      # compute the average of the gradient values, and using them\n",
    "                      # as weights, compute the ponderation of the filters with\n",
    "                      # respect to the weights\n",
    "                        weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n",
    "                        #print(weights.shape)  = 512  (took mean over axis 0  and 1 (14,14))\n",
    "\n",
    "                        \n",
    "                        \n",
    "                        cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n",
    "                        #print(cam.shape) = 512 * (14, 14, 512)  = (14,14)\n",
    "                      # grab the spatial dimensions of the input image and resize\n",
    "                      # the output class activation map to match the input image dimensions\n",
    "                        \n",
    "                       \n",
    "                        (w, h) = (img_array.shape[2], img_array.shape[1])  ## (224,224) here\n",
    "                        heatmap = cv2.resize(cam.numpy(), (w, h))    ## resize (14,14) to (224,224) (upsample)\n",
    "                        \n",
    "                         ### It is good to save the tensors for further analysis\n",
    "                        with open(SAMPLES+'0_'+'loss_check_linear.txt','a') as file:\n",
    "                            print(convOutputs, fc1,fc2,dropout,predictions,loss, grads,castConvOutputs,convOutputs,guidedGrads,\\\n",
    "                                  weights, cam, w,h, sep='\\n' ,file = file)\n",
    "                        \n",
    "                        # normalization\n",
    "                        numer = heatmap - np.min(heatmap)            \n",
    "                        denom = (heatmap.max() - heatmap.min())\n",
    "                        if denom == 0:\n",
    "                            denom = denom + 0.0000000000000001\n",
    "                        heatmap = 255*numer / denom\n",
    "\n",
    "                        Heatmap_2D[224*i:224*(i+1),224*j:224*(j+1)] = heatmap[:,:]\n",
    "                        ## big heatmap made of smaller tiles\n",
    "                        \n",
    "                        \n",
    "                        stacked_img = np.stack((heatmap,), axis=-1) ## \n",
    "                        im_color = cv2.applyColorMap(stacked_img.astype(np.uint8), cv2.COLORMAP_JET) ## add a color map\n",
    "                        im_color = cv2.cvtColor(im_color, cv2.COLOR_BGR2RGB)\n",
    "                        \n",
    "                        Heatmap_only[224*i:224*(i+1),224*j:224*(j+1)] = stacked_img[:,:,:] \n",
    "                        \n",
    "                        \n",
    "                        ## add heatmap to image (0.7 image and 0.3 activation heatmap)\n",
    "                        super_imposed_img = cv2.addWeighted((im_color).astype(np.uint8), 0.3, tmp_img, 0.7,0) \n",
    "                        Gradcam_img[224*i:224*(i+1),224*j:224*(j+1),:] =  super_imposed_img \n",
    "\n",
    "            ### This part is for plot and saving##########################################\n",
    "            \n",
    "            ### Plotting only heatmap\n",
    "            plt.figure(figsize=(15,15))\n",
    "            plt.imshow(Heatmap_2D)\n",
    "            plt.savefig(img_path+'_heatmap.png')\n",
    "           \n",
    "            ### Plotting 3D plot of Heatmap as Z axis\n",
    "            fig   = plt.figure(figsize=(10,10))\n",
    "            ax    = plt.axes(projection='3d')\n",
    "            x     = np.arange(0,max_tile_j*224,1)\n",
    "            y     = np.arange(0,max_tile_i*224,1)\n",
    "            X, Y  = np.meshgrid(x, y)\n",
    "            ax.contour3D(X,Y, Heatmap_only[:,:,0], 100,alpha = 0.5, cmap='jet')\n",
    "            ax.set_xlabel('x')\n",
    "            ax.set_ylabel('y')\n",
    "            ax.set_zlabel('z')\n",
    "            ax.view_init(azim=270, elev=-89)\n",
    "            plt.savefig(img_path+'_3D.png')\n",
    "\n",
    "\n",
    "            \n",
    "            ### Plotting image with GRAD-CAM filter added \n",
    "            plt.figure(figsize=(10,10))\n",
    "            plt.imshow(Gradcam_img.astype(np.uint8))\n",
    "            norm = mpl.colors.Normalize(vmin=Gradcam_img.min(), vmax=Gradcam_img.max())\n",
    "            plt.savefig(img_path+'_img_filter.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_DIR = ['2nd_part']#['1st_part','2nd_part','3rd_part']\n",
    "MODELS         = '../../models/train_more_layers/effnet_aug_3fold_blurred_3_layer_softmax/'\n",
    "SAMPLES        = '../../results/Grad_cam/Train_more_layers/samples_effnet_3layers_softmax/'\n",
    "LAYER_TYPE     =  'sigmoid'\n",
    "LAYER_TYPE_2   =  'sigmoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### This cell is almost the same for Effnet model \n",
    "### Path to model files and also the index of layers are different\n",
    "### we continued to work with this two models and visualize the local activated heatmap\n",
    "\n",
    "val_dirs = VALIDATION_DIR\n",
    "for val_dir in val_dirs:\n",
    "    inputs = tf.keras.Input(shape=(224,224,3))\n",
    "\n",
    "    ### The model is defined here \n",
    "    x = EfficientNetB4(           ### Functional model\n",
    "      input_tensor=inputs, \n",
    "      include_top=False,\n",
    "      weights='imagenet'\n",
    "    )\n",
    "    \n",
    "    x.trainable = False  #The layers are not trainable\n",
    "   \n",
    "    layer_type = LAYER_TYPE\n",
    "    layer_type_fc2  = LAYER_TYPE_2\n",
    "    x = tf.keras.layers.Flatten(name='flatten')(x.output)\n",
    "    x = tf.keras.layers.Dense(128, activation=layer_type, name='fc1')(x)\n",
    "    x = tf.keras.layers.Dense(128, activation=layer_type_fc2, name='fc2')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5, name='dropout')(x)\n",
    "    x = tf.keras.layers.Dense(3, activation='softmax', name='predictions')(x)\n",
    "\n",
    "    ##x.summary()\n",
    "    '''\n",
    "    top_conv (Conv2D)              (None, 7, 7, 1792)   802816      ['block7b_add[0][0]']            \n",
    "                                                                                                  \n",
    "    top_bn (BatchNormalization)    (None, 7, 7, 1792)   7168        ['top_conv[0][0]']               \n",
    "                                                                                                  \n",
    "    top_activation (Activation)    (None, 7, 7, 1792)   0           ['top_bn[0][0]']                 \n",
    "                                                                                                  \n",
    "    flatten (Flatten)              (None, 87808)        0           ['top_activation[0][0]']         \n",
    "                                                                                                  \n",
    "    fc1 (Dense)                    (None, 128)          11239552    ['flatten[0][0]']                \n",
    "                                                                                                  \n",
    "    fc2 (Dense)                    (None, 128)          16512       ['fc1[0][0]']                    \n",
    "                                                                                                  \n",
    "    dropout (Dropout)              (None, 128)          0           ['fc2[0][0]']                    \n",
    "                                                                                                  \n",
    "    predictions (Dense)            (None, 3)            387         ['dropout[0][0]']                \n",
    "                                                                                       \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Same metrics to track here\n",
    "    METRICS = [\n",
    "      keras.metrics.SparseCategoricalAccuracy(),\n",
    "      MulticlassTruePositives()]\n",
    "\n",
    "\n",
    "    #Create the model \n",
    "    pretrained_model = tf.keras.Model(inputs, x)\n",
    "\n",
    "    pretrained_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=METRICS)\n",
    "    \n",
    "    weights = glob.glob(MODELS+'*_best_weights.*')[0]  ## Load weights trained before (Not every model there)\n",
    "   \n",
    "    ### This is important to load the model from trained weights\n",
    "    ### It is necessary to include new objects or classes the defined for the model here\n",
    "    loaded_1 = keras.models.load_model(\n",
    "    weights, custom_objects={\"MulticlassTruePositives\": MulticlassTruePositives,'Functional':EfficientNetB4()})\n",
    "\n",
    "\n",
    "    weights_list = loaded_1.get_weights()\n",
    "    pretrained_model.layers[-1].set_weights([weights_list[-2],weights_list[-1]])\n",
    "    pretrained_model.layers[-3].set_weights([weights_list[-4],weights_list[-3]])\n",
    "    pretrained_model.layers[-4].set_weights([weights_list[-6],weights_list[-5]])\n",
    "    pretrained_model.layers[-7].set_weights([weights_list[-10],weights_list[-9],weights_list[-8],weights_list[-7]])\n",
    "    pretrained_model.layers[-8].set_weights([weights_list[-11]])\n",
    "\n",
    "\n",
    "    model=pretrained_model\n",
    "   \n",
    "    gradModel = Model(\n",
    "          inputs=[pretrained_model.inputs],\n",
    "          outputs=[pretrained_model.get_layer('top_conv').output,pretrained_model.get_layer('fc1').output,\n",
    "                  pretrained_model.output])\n",
    "\n",
    "\n",
    "    imgs = glob.glob(SAMPLES+'main_img_'+val_dir+'/*.tif')\n",
    "\n",
    "    for im in imgs:\n",
    "        img_path = im\n",
    "       \n",
    "        if cv2.imread(img_path) is not None:\n",
    "            input_img = cv2.imread(img_path)\n",
    "            ## This is for chacking all tiles in an image and stitch them together at the end\n",
    "            max_tile_i = int(np.floor(input_img.shape[0]/224))\n",
    "            max_tile_j = int(np.floor(input_img.shape[1]/224))\n",
    "            Heatmap_2D   = np.zeros([max_tile_i*224,max_tile_j*224])\n",
    "            Heatmap_only = np.zeros([max_tile_i*224,max_tile_j*224,1])\n",
    "            Gradcam_img  = np.zeros([max_tile_i*224,max_tile_j*224,3])\n",
    "            img_title = np.zeros([max_tile_i,max_tile_j])\n",
    "            for i in range(max_tile_i):\n",
    "                for j in range(max_tile_j):\n",
    "                    tmp_img = input_img[224*i:224*(i+1),224*j:224*(j+1),:]\n",
    "                    tmp_img = cv2.resize(tmp_img,(224,224))\n",
    "                    y = np.expand_dims(tmp_img, axis=0)\n",
    "                    img_array = y\n",
    "\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        #same as VGG for calculating the gradient of loss w.r.t last convolution layer\n",
    "                        inputs = tf.cast(img_array, tf.float32)\n",
    "                        convOutputs, fc1,predictions = gradModel(inputs)\n",
    "                        #convoutput.shape:  (7, 7, 1792)\n",
    "                        ## We need the predicted class here\n",
    "                        loss = predictions[:, np.argmax(predictions)]\n",
    "                        #loss.shape:  (1,)\n",
    "\n",
    "                        img_title[i,j] = np.argmax(predictions)\n",
    "                        \n",
    "                        # use automatic differentiation to compute the gradients\n",
    "                        grads = tape.gradient(loss, convOutputs)\n",
    "                        #grads.shape:  (1, 7, 7, 1792)\n",
    "\n",
    "\n",
    "                        \n",
    "                        # compute the guided gradients (took the positive part)\n",
    "                        castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n",
    "                        castGrads = tf.cast(grads > 0, \"float32\")\n",
    "                        guidedGrads = castConvOutputs * castGrads * grads \n",
    "                        #guidedgtads.shape:  (1,7, 7, 1792)\n",
    "\n",
    "\n",
    "                       ## Discard the batch dimension\n",
    "                        convOutputs = convOutputs[0]\n",
    "                        guidedGrads = guidedGrads[0]\n",
    "                        #guidedgtads.shape:  (7, 7, 1792)\n",
    "                       \n",
    "                        # compute the average of the gradient values, and using them as weights,\n",
    "                        #compute the effect of fiters with respect to the weights\n",
    "                        weights = tf.reduce_mean(guidedGrads, axis=(0, 1)) \n",
    "                        # weights.shape: (1792,)\n",
    "\n",
    "                        cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n",
    "                        # Cam.shape: (7, 7)\n",
    "                        # grab the spatial dimensions of the input image and resize\n",
    "                        # the output class activation map to match the input image\n",
    "                        # dimensions\n",
    "                        (w, h) = (img_array.shape[2], img_array.shape[1])  \n",
    "                        heatmap = cv2.resize(cam.numpy(), (w, h))\n",
    "                        numer = heatmap - np.min(heatmap)\n",
    "                        denom = (heatmap.max() - heatmap.min())\n",
    "                        if denom == 0:\n",
    "                            denom = denom + 0.0000000000000001\n",
    "                        heatmap = 255*numer / denom\n",
    "\n",
    "                        Heatmap_2D[224*i:224*(i+1),224*j:224*(j+1)] = heatmap[:,:]\n",
    "                        stacked_img = np.stack((heatmap,), axis=-1)\n",
    "                        im_color = cv2.applyColorMap(stacked_img.astype(np.uint8), cv2.COLORMAP_JET)\n",
    "                        im_color = cv2.cvtColor(im_color, cv2.COLOR_BGR2RGB)\n",
    "                        \n",
    "                        Heatmap_only[224*i:224*(i+1),224*j:224*(j+1)] = stacked_img[:,:,:]\n",
    "                        super_imposed_img = cv2.addWeighted((im_color).astype(np.uint8), 0.3, tmp_img, 0.7,0)\n",
    "                        Gradcam_img[224*i:224*(i+1),224*j:224*(j+1),:] =  super_imposed_img \n",
    "\n",
    "\n",
    "            ### This part is for plot and saving##########################################\n",
    "            \n",
    "            ### Plotting only heatmap\n",
    "            plt.figure(figsize=(15,15))\n",
    "            plt.imshow(Heatmap_2D)\n",
    "            plt.savefig(img_path+'_heatmap.png')\n",
    "           \n",
    "            ### Plotting 3D plot of Heatmap as Z axis\n",
    "            fig   = plt.figure(figsize=(10,10))\n",
    "            ax    = plt.axes(projection='3d')\n",
    "            x     = np.arange(0,max_tile_j*224,1)\n",
    "            y     = np.arange(0,max_tile_i*224,1)\n",
    "            X, Y  = np.meshgrid(x, y)\n",
    "            ax.contour3D(X,Y, Heatmap_only[:,:,0], 100,alpha = 0.5, cmap='jet')\n",
    "            ax.set_xlabel('x')\n",
    "            ax.set_ylabel('y')\n",
    "            ax.set_zlabel('z')\n",
    "            ax.view_init(azim=270, elev=-89)\n",
    "            plt.savefig(img_path+'_3D.png')\n",
    "\n",
    "\n",
    "            \n",
    "            ### Plotting image with GRAD-CAM filter added \n",
    "            plt.figure(figsize=(10,10))\n",
    "            plt.imshow(Gradcam_img.astype(np.uint8))\n",
    "            norm = mpl.colors.Normalize(vmin=Gradcam_img.min(), vmax=Gradcam_img.max())\n",
    "            plt.savefig(img_path+'_img_filter.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_DIR = ['2nd_part']  #['1st_part','2nd_part','3rd_part']\n",
    "MODELS         = '../../models/train_more_layers/effnet_aug_3fold_blurred_2_layer_softmax/'\n",
    "SAMPLES        = '../../results/Grad_cam/Train_more_layers/samples_En_m_a_bn_softmax/'\n",
    "LAYER_TYPE     =  'sigmoid'\n",
    "LAYER_TYPE_2   =  'sigmoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This cell is almost the same for Effnet model \n",
    "### Path to model files and also the index of layers are different\n",
    "### we continued to work with this two models and visualize the local activated heatmap\n",
    "\n",
    "val_dirs = VALIDATION_DIR\n",
    "for val_dir in val_dirs:\n",
    "    inputs = tf.keras.Input(shape=(224,224,3))\n",
    "\n",
    "    ### The model is defined here \n",
    "    x = EfficientNetB4(           ### Functional model\n",
    "      input_tensor=inputs, \n",
    "      include_top=False,\n",
    "      weights='imagenet'\n",
    "    )\n",
    "    \n",
    "    x.trainable = False  #The layers are not trainable\n",
    "   \n",
    "    layer_type = LAYER_TYPE\n",
    "    layer_type_fc2  = LAYER_TYPE_2\n",
    "    x = tf.keras.layers.Flatten(name='flatten')(x.output)\n",
    "    x = tf.keras.layers.Dense(128, activation=layer_type, name='fc1')(x)\n",
    "    x = tf.keras.layers.Dense(128, activation=layer_type_fc2, name='fc2')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5, name='dropout')(x)\n",
    "    x = tf.keras.layers.Dense(3, activation='softmax', name='predictions')(x)\n",
    "\n",
    "    ##x.summary()\n",
    "    '''\n",
    "    top_conv (Conv2D)              (None, 7, 7, 1792)   802816      ['block7b_add[0][0]']            \n",
    "                                                                                                  \n",
    "    top_bn (BatchNormalization)    (None, 7, 7, 1792)   7168        ['top_conv[0][0]']               \n",
    "                                                                                                  \n",
    "    top_activation (Activation)    (None, 7, 7, 1792)   0           ['top_bn[0][0]']                 \n",
    "                                                                                                  \n",
    "    flatten (Flatten)              (None, 87808)        0           ['top_activation[0][0]']         \n",
    "                                                                                                  \n",
    "    fc1 (Dense)                    (None, 128)          11239552    ['flatten[0][0]']                \n",
    "                                                                                                  \n",
    "    fc2 (Dense)                    (None, 128)          16512       ['fc1[0][0]']                    \n",
    "                                                                                                  \n",
    "    dropout (Dropout)              (None, 128)          0           ['fc2[0][0]']                    \n",
    "                                                                                                  \n",
    "    predictions (Dense)            (None, 3)            387         ['dropout[0][0]']                \n",
    "                                                                                       \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Same metrics to track here\n",
    "    METRICS = [\n",
    "      keras.metrics.SparseCategoricalAccuracy(),\n",
    "      MulticlassTruePositives()]\n",
    "\n",
    "\n",
    "    #Create the model \n",
    "    pretrained_model = tf.keras.Model(inputs, x)\n",
    "\n",
    "    pretrained_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=METRICS)\n",
    "    \n",
    "    weights = glob.glob(MODELS+'*_best_weights.*')[0]  ## Load weights trained before (Not every model there)\n",
    "   \n",
    "    ### This is important to load the model from trained weights\n",
    "    ### It is necessary to include new objects or classes the defined for the model here\n",
    "    loaded_1 = keras.models.load_model(\n",
    "    weights, custom_objects={\"MulticlassTruePositives\": MulticlassTruePositives,'Functional':EfficientNetB4()})\n",
    "\n",
    "\n",
    "    weights_list = loaded_1.get_weights()\n",
    "    pretrained_model.layers[-1].set_weights([weights_list[-2],weights_list[-1]])\n",
    "    pretrained_model.layers[-3].set_weights([weights_list[-4],weights_list[-3]])\n",
    "    pretrained_model.layers[-4].set_weights([weights_list[-6],weights_list[-5]])\n",
    "    pretrained_model.layers[-7].set_weights([weights_list[-10],weights_list[-9],weights_list[-8],weights_list[-7]])\n",
    "    pretrained_model.layers[-8].set_weights([weights_list[-11]])\n",
    "\n",
    "\n",
    "\n",
    "    model=pretrained_model\n",
    "   \n",
    "    gradModel = Model(\n",
    "          inputs=[pretrained_model.inputs],\n",
    "          outputs=[pretrained_model.get_layer('top_conv').output,pretrained_model.get_layer('fc1').output,\n",
    "                  pretrained_model.output])\n",
    "\n",
    "\n",
    "    imgs = glob.glob(SAMPLES+'main_img_'+val_dir+'/*.tif')\n",
    "\n",
    "    for im in imgs:\n",
    "        img_path = im\n",
    "       \n",
    "        if cv2.imread(img_path) is not None:\n",
    "            input_img = cv2.imread(img_path)\n",
    "            ## This is for chacking all tiles in an image and stitch them together at the end\n",
    "            max_tile_i = int(np.floor(input_img.shape[0]/224))\n",
    "            max_tile_j = int(np.floor(input_img.shape[1]/224))\n",
    "            Heatmap_2D   = np.zeros([max_tile_i*224,max_tile_j*224])\n",
    "            Heatmap_only = np.zeros([max_tile_i*224,max_tile_j*224,1])\n",
    "            Gradcam_img  = np.zeros([max_tile_i*224,max_tile_j*224,3])\n",
    "            img_title = np.zeros([max_tile_i,max_tile_j])\n",
    "            for i in range(max_tile_i):\n",
    "                for j in range(max_tile_j):\n",
    "                    tmp_img = input_img[224*i:224*(i+1),224*j:224*(j+1),:]\n",
    "                    tmp_img = cv2.resize(tmp_img,(224,224))\n",
    "                    y = np.expand_dims(tmp_img, axis=0)\n",
    "                    img_array = y\n",
    "\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        #same as VGG for calculating the gradient of loss w.r.t last convolution layer\n",
    "                        inputs = tf.cast(img_array, tf.float32)\n",
    "                        convOutputs, fc1,predictions = gradModel(inputs)\n",
    "                        #convoutput.shape:  (7, 7, 1792)\n",
    "                        ## We need the predicted class here\n",
    "                        loss = predictions[:, np.argmax(predictions)]\n",
    "                        #loss.shape:  (1,)\n",
    "\n",
    "                        img_title[i,j] = np.argmax(predictions)\n",
    "                        \n",
    "                        # use automatic differentiation to compute the gradients\n",
    "                        grads = tape.gradient(loss, convOutputs)\n",
    "                        #grads.shape:  (1, 7, 7, 1792)\n",
    "\n",
    "\n",
    "                        \n",
    "                        # compute the guided gradients (took the positive part)\n",
    "                        castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n",
    "                        castGrads = tf.cast(grads > 0, \"float32\")\n",
    "                        guidedGrads = castConvOutputs * castGrads * grads \n",
    "                        #guidedgtads.shape:  (1,7, 7, 1792)\n",
    "\n",
    "\n",
    "                       ## Discard the batch dimension\n",
    "                        convOutputs = convOutputs[0]\n",
    "                        guidedGrads = guidedGrads[0]\n",
    "                        #guidedgtads.shape:  (7, 7, 1792)\n",
    "                       \n",
    "                        # compute the average of the gradient values, and using them as weights,\n",
    "                        #compute the effect of fiters with respect to the weights\n",
    "                        weights = tf.reduce_mean(guidedGrads, axis=(0, 1)) \n",
    "                        # weights.shape: (1792,)\n",
    "\n",
    "                        cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n",
    "                        # Cam.shape: (7, 7)\n",
    "                        # grab the spatial dimensions of the input image and resize\n",
    "                        # the output class activation map to match the input image\n",
    "                        # dimensions\n",
    "                        (w, h) = (img_array.shape[2], img_array.shape[1])  \n",
    "                        heatmap = cv2.resize(cam.numpy(), (w, h))\n",
    "                        numer = heatmap - np.min(heatmap)\n",
    "                        denom = (heatmap.max() - heatmap.min())\n",
    "                        if denom == 0:\n",
    "                            denom = denom + 0.0000000000000001\n",
    "                        heatmap = 255*numer / denom\n",
    "\n",
    "                        Heatmap_2D[224*i:224*(i+1),224*j:224*(j+1)] = heatmap[:,:]\n",
    "                        stacked_img = np.stack((heatmap,), axis=-1)\n",
    "                        im_color = cv2.applyColorMap(stacked_img.astype(np.uint8), cv2.COLORMAP_JET)\n",
    "                        im_color = cv2.cvtColor(im_color, cv2.COLOR_BGR2RGB)\n",
    "                        \n",
    "                        \n",
    "                        Heatmap_only[224*i:224*(i+1),224*j:224*(j+1)] = stacked_img[:,:,:]\n",
    "                        super_imposed_img = cv2.addWeighted((im_color).astype(np.uint8), 0.3, tmp_img, 0.7,0)\n",
    "                        Gradcam_img[224*i:224*(i+1),224*j:224*(j+1),:] =  super_imposed_img \n",
    "\n",
    "\n",
    "            ### This part is for plot and saving##########################################\n",
    "            \n",
    "            ### Plotting only heatmap\n",
    "            plt.figure(figsize=(15,15))\n",
    "            plt.imshow(Heatmap_2D)\n",
    "            plt.savefig(img_path+'_heatmap.png')\n",
    "           \n",
    "            ### Plotting 3D plot of Heatmap as Z axis\n",
    "            fig   = plt.figure(figsize=(10,10))\n",
    "            ax    = plt.axes(projection='3d')\n",
    "            x     = np.arange(0,max_tile_j*224,1)\n",
    "            y     = np.arange(0,max_tile_i*224,1)\n",
    "            X, Y  = np.meshgrid(x, y)\n",
    "            ax.contour3D(X,Y, Heatmap_only[:,:,0], 100,alpha = 0.5, cmap='jet')\n",
    "            ax.set_xlabel('x')\n",
    "            ax.set_ylabel('y')\n",
    "            ax.set_zlabel('z')\n",
    "            ax.view_init(azim=270, elev=-89)\n",
    "            plt.savefig(img_path+'_3D.png')\n",
    "\n",
    "\n",
    "            \n",
    "            ### Plotting image with GRAD-CAM filter added \n",
    "            plt.figure(figsize=(10,10))\n",
    "            plt.imshow(Gradcam_img.astype(np.uint8))\n",
    "            norm = mpl.colors.Normalize(vmin=Gradcam_img.min(), vmax=Gradcam_img.max())\n",
    "            plt.savefig(img_path+'_img_filter.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
